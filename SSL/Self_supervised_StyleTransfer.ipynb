{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96RnvQIIU1Qd"
      },
      "source": [
        "# Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7lkTvrewCpf"
      },
      "outputs": [],
      "source": [
        "\n",
        "!git clone https://github.com/danifranco/EM_domain_adaptation.git EM_domain_adaptation\n",
        "!pip install -r EM_domain_adaptation/SSL/requirements.txt\n",
        "!pip install -q git+https://github.com/tensorflow/examples.git\n",
        "import sys\n",
        "sys.path.append('EM_domain_adaptation/SSL')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbF9kwzXVB9i"
      },
      "source": [
        "## Hyperparameters specification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn0reYcwwCph"
      },
      "outputs": [],
      "source": [
        "\n",
        "#GPU selection \n",
        "information_patches=0.05\n",
        "filter_Target=True#@param {type:\"boolean\"}\n",
        "filter_Source=True#@param {type:\"boolean\"}\n",
        "pretrain_model1=True#@param {type:\"boolean\"}\n",
        "GPU_availability=False#@param {type:\"boolean\"}\n",
        "show_images=False#@param {type:\"boolean\"}\n",
        "GPU=\"1\"#Perform and nvidia-smi to check which free gpus are available \n",
        "#Parameters to be modified:\n",
        "plot_history=False#@param {type:\"boolean\"}\n",
        "factor=4 #@param {type:\"integer\"}\n",
        "noise=0.3 #@param {type:\"number\"}\n",
        "factor_patches=1 #@param {type:\"integer\"}\n",
        "random_patches=True#@param {type:\"boolean\"}\n",
        "# === PreTraining parameters ===\n",
        "# number of epochs\n",
        "numEpochsPretrain =  1#@param {type:\"integer\"}\n",
        "# patience\n",
        "patiencePretrain =  1#@param {type:\"integer\"}\n",
        "# learning rate\n",
        "lrPretrain = 5e-4 #@param {type:\"number\"}\n",
        "# batch size\n",
        "batch_size_valuePretrain =  6#@param {type:\"integer\"}\n",
        "# use one-cycle policy for super-convergence? Reduce on plateau?\n",
        "no_schedule = None #@param {type:\"raw\"}\n",
        "schedulePretrain = 'oneCycle' #@param [ \"no_schedule\",\"'oneCycle'\",\"'reduce'\"] {type:\"raw\"}\n",
        "\n",
        "# Network architecture: UNet, ResUNet,MobileNetEncoder\n",
        "model_namePretrain = 'AttentionUNET'#@param ['UNet','MobileNetEncoder','AttentionUNET']\n",
        "# Optimizer name: 'Adam', 'SGD'\n",
        "optimizer_namePretrain = 'Adam'#@param ['Adam','SGD']{type:\"string\"}\n",
        "# Loss function name: 'BCE', 'Dice', 'W_BCE_Dice'\n",
        "loss_acronymPretrain = 'mse' #@param ['mae','mse']{type:\"string\"}\n",
        "max_poolingPretrain=True #@param {type:\"boolean\"}\n",
        "\n",
        "#@title **Training Hyperparameters**\n",
        "\n",
        "# === Training parameters ===\n",
        "# number of epochs\n",
        "numEpochs =  1#@param {type:\"integer\"}\n",
        "# patience\n",
        "patience = 1#@param {type:\"integer\"}\n",
        "# learning rate\n",
        "lr =1e-4#@param {type:\"number\"}\n",
        "# batch size\n",
        "batch_size_value = 5#@param {type:\"integer\"}\n",
        "# use one-cycle policy for super-convergence? Reduce on plateau?\n",
        "schedule = 'oneCycle' #@param [ \"no_schedule\",\"'oneCycle'\",\"'reduce'\"] {type:\"raw\"}\n",
        "# Network architecture: UNet, ResUNet,MobileNetEncoder\n",
        "model_name = 'AttentionUNET' #@param ['UNet','MobileNetEncoder','AttentionUNET']\n",
        "# Optimizer name: 'Adam', 'SGD'\n",
        "optimizer_name = 'Adam' #@param ['Adam','SGD']{type:\"string\"}\n",
        "# Loss function name: 'BCE', 'Dice', 'W_BCE_Dice'\n",
        "loss_acronym = 'BCE' #@param ['BCE','Dice','SEG']{type:\"string\"}\n",
        "# create the network and compile it with its optimizer\n",
        "max_pooling=True #@param {type:\"boolean\"}\n",
        "\n",
        "repetitions=1#@param {type:\"slider\", min:1, max:30, step:1}\n",
        "train_encoder=False #@param {type:\"boolean\"}\n",
        "bottleneck_freezing=False #@param {type:\"boolean\"}\n",
        "train_decoder=True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxktX2wJwCpi"
      },
      "outputs": [],
      "source": [
        "#Select dataset-route {'Lucchi++','Kasthuri++','Achucarro','VNC'}\n",
        "#\n",
        "#Name of the Target and Source folder\n",
        "Target='Lucchi++'#@param ['Lucchi++','Kasthuri++','Achucarro','VNC']\n",
        "Source='Kasthuri++'#@param ['Lucchi++','Kasthuri++','Achucarro','VNC']\n",
        "noisy_input = False #@param {type:\"boolean\"}\n",
        "histogram_matching=True\n",
        "\n",
        "testName=Target+'_'+Source+'_'+model_name \n",
        "\n",
        "#Specify full path for the images\n",
        "train_input_path1 = 'Data/'+Target+'/train/x'\n",
        "train_label_path1 = 'Data/'+Target+'/train/y'\n",
        "test_input_path1 = 'Data/'+Target+'/test/x'\n",
        "test_label_path1 = 'Data/'+Target+'/test/y'\n",
        "\n",
        "train_input_path2 = 'Data/'+Source+'/train/x'\n",
        "train_label_path2 = 'Data/'+Source+'/train/y'\n",
        "test_input_path2 = 'Data/'+Source+'/test/x'\n",
        "test_label_path2 = 'Data/'+Source+'/test/y'\n",
        "\n",
        "train_input_path1_hm = 'Data_hm/'+Target+'/'+Target+'_s-t_'+Source+'/train/x'\n",
        "train_label_path1_hm = 'Data_hm/'+Target+'/'+Target+'_s-t_'+Source+'/train/y'\n",
        "test_input_path1_hm = 'Data_hm/'+Target+'/'+Target+'_s-t_'+Source+'/test/x'\n",
        "test_label_path1_hm ='Data_hm/'+Target+'/'+Target+'_s-t_'+Source+'/test/y'\n",
        "\n",
        "train_input_path2_hm = 'Data_hm/'+Source+'/'+Source+'_s-t_'+Target+'/train/x'\n",
        "train_label_path2_hm ='Data_hm/'+Source+'/'+Source+'_s-t_'+Target+'/train/y'\n",
        "test_input_path2_hm = 'Data_hm/'+Source+'/'+Source+'_s-t_'+Target+'/test/x'\n",
        "test_label_path2_hm = 'Data_hm/'+Source+'/'+Source+'_s-t_'+Target+'/test/y'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "hyperparameters=[['Source','Target','Epochs Pre','patience Pre','lr Pre','batch size Pre','scheduler pre','model','optimizer pre','loss pre','Epochs','Patience','Lr','Batch size','Scheduler','Optimizer','Iterations','Train Encoder','BottleNeck freezing','Train decoder'],[Source,Target,numEpochsPretrain,patiencePretrain,lrPretrain,batch_size_valuePretrain,schedulePretrain,model_name,optimizer_namePretrain,loss_acronymPretrain,numEpochs,patience,lr,batch_size_value,schedule,optimizer_name,repetitions,train_encoder,bottleneck_freezing,train_decoder]]\n",
        "df=pd.DataFrame(hyperparameters)\n",
        "new_header = df.iloc[0] #grab the first row for the header\n",
        "df = df[1:] #take the data less the header row\n",
        "df.columns = new_header \n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbVa7BQ7wCpj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"# Code Execution\n",
        "### Imports and folder creation\n",
        "\"\"\"\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from scipy.sparse.construct import rand\n",
        "\n",
        "from functions import *\n",
        "\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "from scipy.sparse.construct import rand\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#gpu selection \n",
        "gpu_select(GPU_availability,GPU)\n",
        "\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.allow_growth = True\n",
        "# session = tf.Session(config=config)\n",
        "try:\n",
        "  os.mkdir(testName)\n",
        "except:\n",
        " print('Already created folder')\n",
        "os.chdir(testName)\n",
        "try:\n",
        "  os.mkdir('saved_Source_models')\n",
        "except:\n",
        " print('Already created folder')\n",
        "\n",
        "try:\n",
        "  os.mkdir('Plots')\n",
        "except:\n",
        " print('Already created folder')\n",
        "\n",
        "try:\n",
        "  os.mkdir('Test_predictions')\n",
        "except:\n",
        " print('Already created folder')\n",
        "try:\n",
        "  os.mkdir('Prediction2')\n",
        "except:\n",
        " print('Already created folder')\n",
        "\n",
        "try:\n",
        "  os.mkdir('Modelos')\n",
        "except:\n",
        " print('Already created folder')\n",
        " \n",
        "modelos_path='Modelos/'+Target+'-'+Source\n",
        "\n",
        "try:\n",
        "  os.mkdir(modelos_path)\n",
        "except:\n",
        " print('Already created folder')\n",
        "\"\"\"### Pretraining step\n",
        "#### Pretraining step style Dataset 1\n",
        "Load dataset 1 and its corresponding patches\n",
        "\"\"\"\n",
        "\n",
        "nameSavingFile=str((testName+'.xlsx'))\n",
        "\n",
        "\n",
        "#fix the seed to reproduce results\n",
        "set_seed(42)\n",
        "\n",
        "print( tf.__version__ )\n",
        "\n",
        "\n",
        "print('Loading data sets according to style of...'+ Target)\n",
        "##### Loading the Target for pretraining according to \n",
        "# Read the list of file names\n",
        "train_img1,train_lbl1=load_img(train_input_path1,train_label_path1)\n",
        "\n",
        "\n",
        "# display first image\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow( train_img1[0], 'gray' )\n",
        "plt.title( 'Full-size training image' );\n",
        "# and its \"ground truth\"\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow( train_lbl1[0], 'gray' )\n",
        "plt.title( 'Ground truth' );\n",
        "plt.savefig('Plots/'+Target+\"_training_imgs_style\"+Target+\".png\" )\n",
        "\n",
        "#Loading train and validation splits\n",
        "\n",
        "train_img1, val_img1, train_lbl1, val_lbl1 = train_test_split(train_img1,\n",
        "                                                        train_lbl1,\n",
        "                                                        train_size=1-0.1,\n",
        "                                                        test_size=0.1,\n",
        "                                                        random_state=42, shuffle=False)\n",
        "\n",
        "train_img_patches1,train_lbl_patches1=create_patches(train_img1,train_lbl1,(256,256),random_patches=random_patches,factor=factor_patches,filter=filter_Target,threshold=information_patches)\n",
        "\n",
        "\n",
        "val_img_patches1,val_lbl_patches1=create_patches(val_img1,val_lbl1,(256,256),random_patches=random_patches)\n",
        "#del val_img1,val_lbl1\n",
        "num=3\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(1,num+1,2):\n",
        "  \n",
        "  plt.subplot(num, 2, i)\n",
        "  plt.imshow( val_img_patches1[i] ,'gray')\n",
        "  plt.colorbar()\n",
        "  plt.title( 'Input validation patch' );\n",
        "  # and its \"ground truth\"\n",
        "  plt.subplot(num, 2, i+1)\n",
        "  plt.imshow( val_lbl_patches1[i] ,'gray')\n",
        "  plt.title( 'Ground truth patch' );\n",
        "plt.savefig('Plots/'+Target+\"_validation_patches_style\"+Target+\".png\" )\n",
        "\n",
        "# Read the list of file names\n",
        "train_img2,train_lbl2=load_img(train_input_path2_hm,train_label_path2_hm)\n",
        "\n",
        "# display first image\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow( train_img2[0], 'gray' )\n",
        "\n",
        "plt.title( 'Full-size training image' );\n",
        "# and its \"ground truth\"\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow( train_lbl2[0], 'gray' )\n",
        "plt.title( 'Ground truth' );\n",
        "plt.savefig('Plots/'+Source+\"_training_imgs_style\"+Target+\".png\" )\n",
        "\n",
        "\n",
        "\n",
        "#@title\n",
        "#Loading train and validation splits\n",
        "\n",
        "train_img2, val_img2, train_lbl2, val_lbl2 = train_test_split(train_img2,\n",
        "                                                        train_lbl2,\n",
        "                                                        train_size=1-0.1,\n",
        "                                                        test_size=0.1,\n",
        "                                                        random_state=42, shuffle=False)\n",
        "\n",
        "\"\"\"\n",
        "## Preparing the pre-training data\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "percentage_data=1 #parameter to be changed in between 0-1 to reduce  randomly the number of annotated patches to be used during training\n",
        "\n",
        "\n",
        "\n",
        "num=3\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(1,num+1,2):\n",
        "  \n",
        "  plt.subplot(num, 2, i)\n",
        "  plt.imshow( train_img_patches1[i] ,'gray')\n",
        "  plt.colorbar()\n",
        "  plt.title( 'Input training patch' );\n",
        "  # and its \"ground truth\"\n",
        "  plt.subplot(num, 2, i+1)\n",
        "  plt.imshow( train_lbl_patches1[i] ,'gray')\n",
        "  plt.title( 'Ground truth patch' );\n",
        "plt.savefig('Plots/'+Target+\"_training_patches_style\"+Target+\".png\" )\n",
        "\n",
        "\n",
        "\n",
        "train_img_patches2,train_lbl_patches2=create_patches(train_img2,train_lbl2,(256,256),random_patches=random_patches,factor=factor_patches,filter=filter_Source,threshold=information_patches)\n",
        "#del train_img2,train_lbl2\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(1,num+1,2):\n",
        "  \n",
        "  plt.subplot(num, 2, i)\n",
        "  plt.imshow( train_img_patches2[i] ,'gray')\n",
        "  plt.colorbar()\n",
        "  plt.title( 'Input training patch' );\n",
        "  # and its \"ground truth\"\n",
        "  plt.subplot(num, 2, i+1)\n",
        "  plt.imshow( train_lbl_patches2[i] ,'gray')\n",
        "  plt.title( 'Ground truth patch' );\n",
        "plt.savefig('Plots/'+Source+\"_training_patches_style\"+Target+\".png\")\n",
        "\n",
        "\n",
        "val_img_patches2,val_lbl_patches2=create_patches(val_img2,val_lbl2,(256,256),random_patches=random_patches)\n",
        "#del val_img2,val_lbl2\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(1,num+1,2):\n",
        "  \n",
        "  plt.subplot(num, 2, i)\n",
        "  plt.imshow( val_img_patches2[i] ,'gray')\n",
        "  plt.colorbar()\n",
        "  plt.title( 'Input validation patch' );\n",
        "  # and its \"ground truth\"\n",
        "  plt.subplot(num, 2, i+1)\n",
        "  plt.imshow( val_lbl_patches2[i] ,'gray')\n",
        "  plt.title( 'Ground truth patch' );\n",
        "plt.savefig('Plots/'+Source+\"_validation_patches_style\"+Target+\".png\")\n",
        "\n",
        "#### Self-supervised pretraining\n",
        "\n",
        "#Create test patches to include in the pretraining\n",
        "#@title\n",
        "\"\"\"Now we will check the number of images and masks:\"\"\"\n",
        "\n",
        "test_img1,test_lbl1=load_img(test_input_path1,test_label_path1)\n",
        "\n",
        "# Display corresponding first patch at low resolution\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow( test_img1[0], 'gray' )\n",
        "plt.title( 'Test image' )\n",
        "# Side by side with its \"ground truth\"\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow( test_lbl1[0], 'gray' )\n",
        "plt.title( 'Ground truth' )\n",
        "plt.savefig('Plots/'+Target+\"_test_imgs_style\"+Target+\".png\" )\n",
        "\n",
        "# Read the list of file names\n",
        "test_img2,test_lbl2=load_img(test_input_path2_hm,test_label_path2_hm)\n",
        "\n",
        "\n",
        "# Display corresponding first patch at low resolution\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow( test_img2[0], 'gray' )\n",
        "plt.title( 'Test image' )\n",
        "# Side by side with its \"ground truth\"\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow( test_lbl2[0], 'gray' )\n",
        "plt.title( 'Ground truth' )\n",
        "plt.savefig('Plots/'+Source+\"_test_imgs_style\"+Target+\".png\" )\n",
        "\n",
        "test_img_patches1,test_lbl_patches1=create_patches(test_img1,test_lbl1,(256,256),random_patches=random_patches,factor=factor_patches,filter=filter_Target,threshold=information_patches)\n",
        "\n",
        "test_img_patches2,test_lbl_patches2=create_patches(test_img2,test_lbl2,(256,256),random_patches=random_patches,factor=factor_patches,filter=filter_Source,threshold=information_patches)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#from SelfSupervisedLearning.general_functions import add_Gaussian_Noise,crappify\n",
        " #Here we mix both datasets so that we can perform the pretraining step\n",
        "train_img_patches=train_img_patches1+train_img_patches2+test_img_patches1+test_img_patches2\n",
        "val_img_patches=val_img_patches1+val_img_patches2\n",
        "size_train1=len(train_img_patches1)\n",
        "#del train_img_patches1,train_img_patches2,test_img_patches1,test_img_patches2,val_img_patches1,val_img_patches2\n",
        "\n",
        "if pretrain_model1:\n",
        "  \n",
        "    \"\"\"**There are 3 options to perform the pretraining step:**\n",
        "    * `hide_fragments`: is meant to perform inpainting it hides fragments of the image by setting them to zero, we can choose the size of the fragments as well as the percentage of the image hidden\n",
        "    * `add_Gaussian_Noise`: is meant to perform denoising by adding Normal Gaussian Noise N(0,$\\sigma$) to the images. \n",
        "    * `crappify`: is meant to simulate superresolution so that we start by adding noise to the image and then downsizing and upsizing it.\n",
        "    \"\"\"\n",
        "\n",
        "    #@title\n",
        "    # We will use these patches as \"ground truth\" for the pretraining step\n",
        "    noisy_train_img=[crappify(x,resizing_factor=factor,add_noise=True,noise_level=noise) for x in train_img_patches]\n",
        "    X_train,Y_train=prepare_training_data(noisy_train_img,train_img_patches)\n",
        "    #del noisy_train_img,train_img_patches\n",
        "    noisy_val_img=[crappify(x,resizing_factor=factor,add_noise=True,noise_level=noise) for x in val_img_patches]\n",
        "    X_val,Y_val=prepare_training_data(noisy_val_img,val_img_patches)\n",
        "    #del noisy_val_img,val_img_patches\n",
        "\n",
        "    \"\"\"###Pretraining - Super resolution\n",
        "    The idea is to **pretrain the network by using the noisy patches** previously created as the input and the **pretraining ground_truth would be the original images**. \n",
        "    This procedure is meant to **enhance the initial weights of our model to afterwards improve its segmentation performance and its transferability to the target domain**.\n",
        "    As loss function, we use the mean squared error (MSE) between the expected and the predicted pixel values, and we also include the mean absolute error (MAE) as a control metric.\n",
        "    For this step **we will use all the training data**, as even simulating scarcity of labelled data, the unlabelled data might still be available for being used in SSL super resolution.\n",
        "    Furthermore we will evaluate the PSNR SSNR\n",
        "    \"\"\"\n",
        "\n",
        "    #@title\n",
        "    # Prepare the training data and create data generators\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    evaluate_ranges(X_train)\n",
        "    evaluate_ranges(Y_train)\n",
        "    evaluate_ranges(X_val)\n",
        "    evaluate_ranges(Y_val)\n",
        "\n",
        "\n",
        "\n",
        "    # Display corresponding first 3 patches\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.imshow( Y_train[0,:,:,0], 'gray' )\n",
        "    plt.title( ' Original patch from '+Target )\n",
        "    # Side by side with its \"ground truth\"\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.imshow( X_train[0,:,:,0], 'gray' )\n",
        "    plt.title( ' Noisy training patch from '+Target )\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.imshow( Y_train[size_train1+3,:,:,0], 'gray' )\n",
        "    plt.title( ' Original patch from '+Source )\n",
        "    # Side by side with its \"ground truth\"\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.imshow( X_train[size_train1+3,:,:,0], 'gray' )\n",
        "    plt.title( 'Noisy training patch from '+Source  )\n",
        "\n",
        "    plt.savefig('Plots/'+\"Pretraining_patches_imgs_style\"+Target+\".png\" )\n",
        "\n",
        "    #@title\n",
        "\n",
        "  \n",
        "    for ii in range(repetitions):\n",
        "\n",
        "      tf.keras.backend.clear_session()\n",
        "      history,model=train(X_train,Y_train,X_val,Y_val,numEpochsPretrain,1,patiencePretrain,lrPretrain,lrPretrain,batch_size_valuePretrain,schedulePretrain,model_name,optimizer_namePretrain,loss_acronymPretrain,max_poolingPretrain,train_encoder=True,plot_history=plot_history,Denoising=True,preTrain=True,check_ev=False,path_save='saved_denoised_models')\n",
        "      print('Pre-trained weights are ready to be used!')\n",
        "\n",
        "      model.save_weights(modelos_path+'/Pretrained_model_style_'+Target+'_'+str(ii)+'.h5')\n",
        "\n",
        "      #Evaluar métricas denoising sobre entrenamiento, validación y test\n",
        "\n",
        "      psnr_array_noise = []\n",
        "\n",
        "\n",
        "      print('\\n# Generate predictions for all validation samples')\n",
        "\n",
        "      for i in range(0, len(X_val)):\n",
        "        pred = X_val[i][:,:,0];\n",
        "        psnr_array_noise.append(metrics.peak_signal_noise_ratio(pred, Y_val[i][:,:,0]));\n",
        "      psnr_mean_noise = np.mean(psnr_array_noise)\n",
        "\n",
        "      ssim_array_noise = []\n",
        "      for i in range(0, len(X_val)):\n",
        "        pred = X_val[i][:,:,0];\n",
        "        ssim_array_noise.append(metrics.structural_similarity(pred, Y_val[i][:,:,0]));\n",
        "      ssim_mean_noise = np.mean(ssim_array_noise)\n",
        "\n",
        "      print(\"PSNR original:\", psnr_mean_noise)\n",
        "      print(\"SSIM original:\", ssim_mean_noise)\n",
        "      file1 = open(testName+'.txt',\"a\")\n",
        "      import pandas as pd\n",
        "      # \\n is placed to indicate EOL (End of Line)\n",
        "      file1.write(Target+\"PSNR original: \"+ str(psnr_mean_noise)+'\\n')\n",
        "      file1.write(Target+\"SSIM original:\"+ str(ssim_mean_noise)+'\\n')\n",
        "      file1.close() #to change file access modes\n",
        "    \n",
        "      #Evaluar métricas denoising sobre entrenamiento, validación y test\n",
        "\n",
        "      psnr_array = []\n",
        "\n",
        "\n",
        "      print('\\n# Generate predictions for all validation samples')\n",
        "      predictions = model.predict(X_val)\n",
        "      for i in range(0, len(predictions)):\n",
        "        pred = np.clip( predictions[i][:,:,0], a_min=0, a_max=1 );\n",
        "        psnr_array.append(metrics.peak_signal_noise_ratio(pred, Y_val[i][:,:,0]));\n",
        "      psnr_mean = np.mean(psnr_array)\n",
        "\n",
        "      ssim_array = []\n",
        "      for i in range(0, len(predictions)):\n",
        "        pred = np.clip( predictions[i][:,:,0], a_min=0, a_max=1 );\n",
        "        ssim_array.append(metrics.structural_similarity(pred, Y_val[i][:,:,0]));\n",
        "      ssim_mean = np.mean(ssim_array)\n",
        "\n",
        "      print(\"PSNR reconstructed:\", psnr_mean)\n",
        "      print(\"SSIM reconstructed:\", ssim_mean)\n",
        "      # \\n is placed to indicate EOL (End of Line)\n",
        "      file1 = open(testName+'.txt',\"a\")\n",
        "\n",
        "      file1.write(\"PSNR reconstructed: \"+ str(psnr_mean)+'\\n')\n",
        "      file1.write(\"SSIM reconstructed:\"+ str(ssim_mean)+'\\n')\n",
        "      file1.close() #to change file access modes\n",
        "      \n",
        "      \"\"\"Now it would be interesting to visualize our output data to check what does the output of our net look like. \"\"\"\n",
        "\n",
        "        #@title\n",
        "      try:\n",
        "        print('predictions shape:', predictions.shape)\n",
        "        # Display corresponding first 3 patches\n",
        "        plt.figure(figsize=(15,15))\n",
        "        plt.subplot(3, 3, 1)\n",
        "        plt.imshow( Y_val[0,:,:,0], 'gray' )\n",
        "        plt.title( 'Validation original image' )\n",
        "        # Side by side with its \"ground truth\"\n",
        "        plt.subplot(3, 3, 2)\n",
        "        plt.imshow( X_val[0,:,:,0], 'gray' )\n",
        "        plt.title( 'Image with added noise' )\n",
        "        # and its prediction\n",
        "        plt.subplot(3, 3, 3)\n",
        "        plt.imshow( predictions[0,:,:,0], cmap='gray' )\n",
        "        plt.title( 'Denoised image' )\n",
        "\n",
        "        plt.subplot(3, 3, 4)\n",
        "        plt.imshow( Y_val[size_train1+3,:,:,0], 'gray' )\n",
        "        plt.title( 'Validation original image' )\n",
        "        # Side by side with its \"ground truth\"\n",
        "        plt.subplot(3, 3, 5)\n",
        "        plt.imshow( X_val[size_train1+3,:,:,0], 'gray' )\n",
        "        plt.title( 'Image with added noise' )\n",
        "        # and its prediction\n",
        "        plt.subplot(3, 3, 6)\n",
        "        plt.imshow( predictions[size_train1+3,:,:,0], cmap= 'gray' )\n",
        "        plt.title( 'Denoised image' )\n",
        "\n",
        "        plt.subplot(3, 3, 7)\n",
        "        plt.imshow( Y_val[-1,:,:,0], 'gray' )\n",
        "        plt.title( 'Validation original image' )\n",
        "        # Side by side with its \"ground truth\"\n",
        "        plt.subplot(3, 3, 8)\n",
        "        plt.imshow( X_val[-1,:,:,0], 'gray' )\n",
        "        plt.title( 'Image with added noise' )\n",
        "        # and its prediction\n",
        "        plt.subplot(3, 3, 9)\n",
        "        plt.imshow( predictions[-1,:,:,0], cmap= 'gray' )\n",
        "        plt.title( 'Denoised image' )\n",
        "        plt.savefig('Plots/'+'Pretraining_predictions_style'+Target+str(ii)+'.png')\n",
        "      except:\n",
        "        print('Problem when printing superresolution predictions it requires to use X_train,X_val...')\n",
        "\n",
        "      #del model\n",
        "      ##del X_train,X_val,Y_train,Y_val\n",
        "\n",
        "train_img2,train_lbl2=load_img(train_input_path2_hm,train_label_path2_hm)\n",
        "\n",
        "train_img2, val_img2, train_lbl2, val_lbl2 = train_test_split(train_img2,\n",
        "                                                        train_lbl2,\n",
        "                                                        train_size=1-0.1,\n",
        "                                                        test_size=0.1,\n",
        "                                                        random_state=42, shuffle=False)\n",
        "\n",
        "train_img_patches2,train_lbl_patches2=create_patches(train_img2,train_lbl2,(256,256),random_patches=random_patches,factor=factor_patches,filter=filter_Source,threshold=information_patches)\n",
        "\n",
        "if noisy_input:train_img_patches2=[crappify(x,resizing_factor=factor,add_noise=True,noise_level=noise) for x in train_img_patches2]\n",
        "#del train_img2,train_lbl2\n",
        "\n",
        "val_img_patches2,val_lbl_patches2=create_patches(val_img2,val_lbl2,(256,256),random_patches=random_patches)\n",
        "if noisy_input:val_img_patches2=[crappify(x,resizing_factor=factor,add_noise=True,noise_level=noise) for x in val_img_patches2]\n",
        "#del val_img2,val_lbl2\n",
        "#@title\n",
        "X_train,Y_train=prepare_training_data(train_img_patches2,train_lbl_patches2)\n",
        "X_val,Y_val=prepare_training_data(val_img_patches2,val_lbl_patches2)\n",
        "#del train_img_patches2,train_lbl_patches2,val_img_patches2,val_lbl_patches2\n",
        "#@title\n",
        "evaluate_ranges(X_train)\n",
        "evaluate_ranges(Y_train)\n",
        "evaluate_ranges(X_val)\n",
        "evaluate_ranges(Y_val)\n",
        "\n",
        "\n",
        "#@title\n",
        "\n",
        "total_seg100=[]\n",
        "total_prec100=[]\n",
        "#from SelfSupervisedLearning.DenoiSeg_functions import threshold_optimization\n",
        "test_img1,test_lbl1=load_img(test_input_path1,test_label_path1)\n",
        "if noisy_input:test_img1=[crappify(x,resizing_factor=factor,add_noise=True,noise_level=noise) for x in test_img1]\n",
        "X_test,Y_test,test_lbl=prepare_test_data(test_img1,test_lbl1)\n",
        "\n",
        "#del test_img1,test_lbl1\n",
        "\n",
        "for i in range(0,repetitions):\n",
        "    model = modelos_path+'/Pretrained_model_style_'+Target+'_'+str(i)+'.h5'\n",
        "    tf.keras.backend.clear_session()\n",
        "    history,model2=train(X_train,Y_train,X_val,Y_val,numEpochs,1,patience,lr,lr*1e-1,batch_size_value,schedule,model_name,optimizer_name,loss_acronym,max_pooling,train_encoder=train_encoder,preTrain=False,Denoising=False,pre_load_weights=True,pretrained_model=model,plot_history=plot_history,bottleneck_freezing=bottleneck_freezing,check_ev=True,path_save='saved_Source_models',X_test=X_test,Y_test=Y_test,train_decoder=train_decoder,Source=Source,Target=Target)\n",
        "    # Evaluate the model on the test data using `evaluate`\n",
        "    model2.save_weights(Source+'TO'+Target+'_FineTunedModel'+str(i)+'.h5')\n",
        "    print('\\n# Evaluate on test data with all training data in loop:',i)\n",
        "\n",
        "#del X_train,Y_train,X_val,Y_val,X_test,test_lbl\n",
        "test_img2,test_lbl2=load_img(test_input_path2_hm,test_label_path2_hm)\n",
        "if noisy_input:test_img2=[crappify(x,resizing_factor=factor,add_noise=True,noise_level=noise) for x in test_img2]\n",
        "X_test,_,test_lbl=prepare_test_data(test_img2,test_lbl2)\n",
        "#del test_img2,test_lbl2\n",
        "\n",
        "IoU_Source2Source=[]\n",
        "model_input_filenames2 = [x for x in os.listdir() if x.endswith(\".h5\") and x.startswith(Source)]\n",
        "for w in  model_input_filenames2 :\n",
        "  model2.load_weights(w)\n",
        "  IoU_Source2Source.append(evaluate_test(X_test,test_lbl,model2))\n",
        "  \n",
        "\n",
        "print('The average SEG in test set is: ',IoU_Source2Source)\n",
        "df['IoU Source-Source']=np.mean(IoU_Source2Source)\n",
        "df['Std Source-Source']=np.std(IoU_Source2Source)\n",
        "try:\n",
        "    file1 = open(testName + '.txt',\"a\")\n",
        "    file1.write('Source-Source IoU:')\n",
        "    file1.write(str(np.mean(IoU_Source2Source))+'\\n')\n",
        "    file1.write('Source-Source std:')\n",
        "    file1.write(str(np.std(IoU_Source2Source))+'\\n')\n",
        "    \n",
        "\n",
        "   \n",
        "except:\n",
        "    print('No se ha podido copiar en el txt el IoU en el dataset 2')\n",
        "#@title\n",
        "file1.close() #to change file access modes\n",
        "predictions=[]\n",
        "for i in range(0,len(X_test[0:5])):\n",
        "      #print('Evaluating test image',i)\n",
        "      normalizedImg = X_test[i][:,:,:]\n",
        "      prediction = model2.predict(normalizedImg[np.newaxis,:,:]);\n",
        "      image=prediction[0,:,:,:]>0.5;\n",
        "      predictions.append(image);\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(2,3,1)\n",
        "plt.imshow(X_test[0][:,:,0], 'gray')\n",
        "plt.title('Original image')\n",
        "plt.subplot(2,3,2)\n",
        "plt.imshow(predictions[0][:,:,0], 'gray')\n",
        "plt.title('Predicted labels')\n",
        "plt.subplot(2,3,3)\n",
        "plt.imshow(test_lbl[0].astype(int)[:,:], 'gray')\n",
        "plt.title('GT labels')\n",
        "plt.subplot(2,3,4)\n",
        "plt.imshow(X_test[len(predictions)-1][:,:,0], 'gray')\n",
        "plt.title('Original image')\n",
        "plt.subplot(2,3,5)\n",
        "plt.imshow(predictions[len(predictions)-1][:,:,0], 'gray')\n",
        "plt.title('Predicted labels')\n",
        "plt.subplot(2,3,6)\n",
        "plt.imshow(test_lbl[len(predictions)-1].astype(int)[:,:], 'gray')\n",
        "plt.title('GT labels')\n",
        "plt.suptitle(Source+' trained model to '+Source +'\\n with IoU:'+str(np.mean(IoU_Source2Source))+'$\\pm$'+str(np.std(IoU_Source2Source)))\n",
        "\n",
        "plt.savefig('Plots/'+'Model_Source_Predictions_Source_hm.png')\n",
        "#del X_test,Y_test,test_lbl\n",
        "test_img1,test_lbl1=load_img(test_input_path1,test_label_path1)\n",
        "if noisy_input:test_img1=[crappify(x,resizing_factor=factor,add_noise=True,noise_level=noise) for x in test_img1]\n",
        "X_test,Y_test,test_lbl=prepare_test_data(test_img1,test_lbl1)\n",
        "#del test_img1,test_lbl1\n",
        "\n",
        "IoU_Source2Target=[]\n",
        "for w in  model_input_filenames2 :\n",
        "  model2.load_weights(w)\n",
        "\n",
        "  IoU_Source2Target.append(evaluate_test(X_test,test_lbl,model2,save_img=True,path='Test_predictions'))\n",
        "\n",
        "print('The average SEG in test set is: ',np.mean(IoU_Source2Target))\n",
        "df['IoU Source-Target']=np.mean(IoU_Source2Target)\n",
        "df['Std Source-Target']=np.std(IoU_Source2Target)\n",
        "\n",
        "try:\n",
        "    file1 = open(testName + '.txt',\"a\")\n",
        "    file1.write('Source-Target IoU:')\n",
        "    file1.write(str(np.mean(IoU_Source2Target))+'\\n')\n",
        "    file1.write('Source-Target std:')\n",
        "    file1.write(str(np.std(IoU_Source2Target))+'\\n')\n",
        "\n",
        "    \n",
        "except:\n",
        "    print('No se ha podido copiar en el txt el IoU en el dataset 2')\n",
        "#@title\n",
        "file1.close() #to change file access modes\n",
        "predictions=[]\n",
        "for i in range(0,len(X_test)):\n",
        "    #print('Evaluating test image',i)\n",
        "    normalizedImg = X_test[i][:,:,:]\n",
        "    prediction = model2.predict(normalizedImg[np.newaxis,:,:]);\n",
        "    image=prediction[0,:,:,:];\n",
        "    predictions.append(image);\n",
        "try:\n",
        "  gr_im= Image.fromarray((predictions[0][:,:,0]*255).astype(np.uint8)).save(Source+'TO'+Target+'image.png')\n",
        "except:\n",
        "  print('Gray scale image hasnt been saved')\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(X_test[0][:,:,0], 'gray')\n",
        "plt.savefig('Plots/'+'Model_Source_img.png')\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(predictions[0][:,:,0], 'gray')\n",
        "plt.savefig('Plots/'+'Model_Source_Prediction.png')\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(2,3,1)\n",
        "plt.imshow(X_test[0][:,:,0], 'gray')\n",
        "plt.title('Original image')\n",
        "plt.subplot(2,3,2)\n",
        "plt.imshow(predictions[0][:,:,0], 'gray')\n",
        "plt.title('Predicted labels')\n",
        "plt.subplot(2,3,3)\n",
        "plt.imshow(test_lbl[0].astype(int)[:,:], 'gray')\n",
        "plt.title('GT labels')\n",
        "plt.subplot(2,3,4)\n",
        "plt.imshow(X_test[len(predictions)-1][:,:,0], 'gray')\n",
        "plt.title('Original image')\n",
        "plt.subplot(2,3,5)\n",
        "plt.imshow(predictions[len(predictions)-1][:,:,0], 'gray')\n",
        "plt.title('Predicted labels')\n",
        "plt.subplot(2,3,6)\n",
        "plt.imshow(test_lbl[len(predictions)-1].astype(int)[:,:], 'gray')\n",
        "plt.title('GT labels')\n",
        "plt.suptitle(Source+' trained model to '+Target +'\\n with IoU:'+str(np.mean(IoU_Source2Target))+'$\\pm$'+str(np.std(IoU_Source2Target)))\n",
        "plt.savefig('Plots/'+'Model_Source_Predictions_Target.png')\n",
        "df.to_csv(testName+'Resultados_hm.csv',mode='a',header=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "SSL__StyleTransfer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}